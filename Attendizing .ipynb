{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students list\n",
      "['Abdulrahman Alamer', 'Hussain Alobaidi', 'Mohammed Alshamasi', 'mustafa alolaiwat']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import dlib\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from playsound import playsound\n",
    "#######################################Face Recognition###############################################\n",
    "\n",
    "#Take the images from file and encoding them: \n",
    "path = 'faces'\n",
    "images = []\n",
    "classNames = []\n",
    "myList = os.listdir(path)\n",
    "\n",
    "for cl in myList:                                #Looping through the file and take the images.\n",
    "    curImg = cv2.imread(f'{path}/{cl}')\n",
    "    images.append(curImg)\n",
    "    classNames.append(os.path.splitext(cl)[0])  # split the name of person from the image name\n",
    "print('Students list')\n",
    "print(classNames)  #print all names\n",
    "\n",
    "#images encoding:\n",
    "encodeList = []\n",
    "for img in images:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    encode = face_recognition.face_encodings(img)[0]\n",
    "    encodeList.append(encode)\n",
    "encodeListKnown =encodeList\n",
    "\n",
    "######################################################################################################\n",
    "#############################Drowsiness_Detection#####################################################\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "thresh = 0.25\n",
    "frame_check = 20\n",
    "detect = dlib.get_frontal_face_detector()\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "flag=0\n",
    "#####################################Emotion analysis#################################################\n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "model = model_from_json(open(\"model.json\", \"r\").read())\n",
    "#load weights\n",
    "model.load_weights('model.h5')\n",
    "################################################Emotion Detection#######################################################    \n",
    "#open webcam:\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:  \n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "\n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        #find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (400, 400))\n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "    \n",
    "\n",
    "#######################################Face Recognition###############################################\n",
    "\n",
    "\n",
    "    success, img = cap.read()  #take captures from webcam\n",
    "  \n",
    "    #find faces from webcam and compare it with our images in file:\n",
    "    imgS = cv2.resize(img,(0,0),None,0.25,0.25)\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "    facesCurFrame = face_recognition.face_locations(imgS)\n",
    "    encodesCurFrame = face_recognition.face_encodings(imgS,facesCurFrame)\n",
    "    for encodeFace,faceLoc in zip(encodesCurFrame,facesCurFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown,encodeFace)\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown,encodeFace)\n",
    "        #print(faceDis)\n",
    "        matchIndex = np.argmin(faceDis) # to find best match\n",
    "        if matches[matchIndex]:\n",
    "            name = classNames[matchIndex].upper()\n",
    "            \n",
    "            #to determine the Faces by rectungle with names in the bottom:\n",
    "            y1,x2,y2,x1 = faceLoc\n",
    "            y1, x2, y2, x1 = y1*4,x2*4,y2*4,x1*4\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "            cv2.rectangle(img,(x1,y2-35),(x2,y2),(0,255,0),cv2.FILLED)\n",
    "            cv2.putText(img,name,(x1+6,y2-6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2)\n",
    "            \n",
    "            # Record the attendence in Excel:\n",
    "            with open('Attendance_List1.csv','r+') as f:\n",
    "                myDataList = f.readlines()\n",
    "                nameList = []\n",
    "                for line in myDataList:\n",
    "                    entry = line.split(',')\n",
    "                    nameList.append(entry[0])\n",
    "                if name not in nameList:\n",
    "                    now = datetime.now()\n",
    "                    dtString = now.strftime('%d/%b/%Y, %H:%M:%S')\n",
    "                        #f.writelines(f'\\n{name},{dtString}')\n",
    "                    today8am = now.replace(hour=7, minute=0, second=0, microsecond=0)\n",
    "                    if now < today8am:\n",
    "                        f.writelines(f'\\n{name},{dtString},NO,{predicted_emotion}')\n",
    "                    else:\n",
    "                        f.writelines(f'\\n{name},{dtString},YES,{predicted_emotion}')\n",
    "   \n",
    "                      \n",
    "    cv2.imshow('Attendence',img)\n",
    "\n",
    "#############################Drowsiness_Detection#####################################################\n",
    "\n",
    "    ret, frame=cap.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    subjects = detect(gray, 0)\n",
    "    for subject in subjects:\n",
    "        shape = predict(gray, subject)\n",
    "        shape = face_utils.shape_to_np(shape)#converting to NumPy Array\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "\n",
    "\n",
    "        if ear < thresh:\n",
    "            flag += 1\n",
    "\n",
    "            if flag >= frame_check:               \n",
    "                cv2.putText(frame, \"****************Wake Up!****************\", (10, 30),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"****************Wake Up!****************\", (10,325),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                wavFile = \"alarm.wav \"\n",
    "                playsound(wavFile)\n",
    "        else:\n",
    "            flag = 0\n",
    "    cv2.imshow(\"Drowsniss Alert\", frame)\n",
    "    # close window when Esc button pressed:\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
